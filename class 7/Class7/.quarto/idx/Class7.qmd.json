{"title":"Class 7: Machine Learning 1","markdown":{"yaml":{"title":"Class 7: Machine Learning 1","author":"Michael Romero (A18135877)","format":"html"},"headingText":"K-means clustering","containsRefs":false,"markdown":"\n\nToday we will explore some fundamental machine learning methods including clustering and dimensionality reduction.\n\n\nTo see how this works let's first makeup some data to cluster where we know what the answer should be. We can use the `rnorm()` function to help here:\n\n```{r}\nhist( rnorm(500, mean=5))\n```\n\n\n```{r}\nx<- c(rnorm(30, mean=-3),rnorm(30, mean=3))\ny<- rev(x)\n```\n\n```{r}\nx<- cbind(x,y)\nplot(x)\n```\n\nThe function for K-means clustering in \"base\" R is `kmeans()` \n\n```{r}\nk<-kmeans(x,centers=2)\nk\n```\n\nTo get ath the results fo the returned list object we can use the dollar `$` syntax\n\n>Q. How many points are in each cluster? 30\n\n```{r}\nk$size\n```\n\n>Q. What 'component' of your result object details \n      - cluster assignment/membership?\n      - cluster center?\n      \n```{r}\nk$cluster\n```\n```{r}\nk$centers\n```\n\n>Q. Make a clustering results figure of the data colored by cluster membership. \n\n```{r}\nplot(x, col=k$cluster, pch=16)\npoints(k$centers, col=\"blue\", pch=15, cex=2)\n```\n\nK-means clustering is very popular as it is very fast and relatively straight forward: it takes numeric data as input and returns the clusterm membership vector etc.\n\nThe \"issue\" is we tell `kmeans()` how many clusters we want!\n\n> Q. Run kmeans again and cluster into 4 groups/clusters and plot the results like we did above. \n\n```{r}\nk4<-kmeans(x, centers = 4)\nplot(x, col=k4$cluster)\npoints(k4$centers, pch=15)\n```\n\nScree plot to pick k `centers` value\n\nbrute-force\n\n```{r}\nk1<-kmeans(x, centers=1)\nk2<-kmeans(x, centers=2)\nk3<-kmeans(x, centers=3)\nk4<-kmeans(x, centers=4)\nk5<-kmeans(x, centers=5)\n```\n\n```{r}\nz<-c(k1$tot.withinss,\n  k2$tot.withinss,\n  k3$tot.withinss,\n  k4$tot.withinss,\n  k5$tot.withinss)\n\nplot(z, typ=\"b\")\n```\n\n```{r}\nn<-NULL\nfor(i in 1:5){\n  n<- c(n, kmeans(x, centers=i)$tot.withinss)\n}\n\nplot(n, typ=\"b\")\n```\n\n\nElbow point is the region after the \"scree\" \"cliff-fall\" so in this case it is at cluster=2.\n\n##Hierarchical Clustering\n\nThe main \"base\" R function for Hierarchical Clustering is called `hclust()`. Here we can't just input our data we need to first calculate a distance matrix (e.g. `dist()`) for our data and use this as input to `hclust()`\n\n```{r}\nd<-dist(x)\nhc<-hclust(d)\nhc\n```\n\nThere is a plot method for hclust results lets try it\n\n```{r}\nplot(hc)\nabline(h=8, col=\"red\")\ncutree(hc, h=8)\n```\nTo get our cluster \"membership\" vector (i.e. our main clustering result) we can \"cut\" the tree at a given height or at a height that yields a given \"k\" groups.\n\n```{r}\ncutree(hc,h=8)\n```\n\n```{r}\ngrps<-cutree(hc, k=2)\n```\n\n> Q. Plot the data with our hclust result coloring\n\n```{r}\nplot(x, col=grps)\n```\n\n# Principal Component Analysis (PCA)\n\n## PCA of UK food data\n\nImport food data from an online CSV file:\n\n```{r}\nurl<- \"https://tinyurl.com/UK-foods\"\nx<- read.csv(url)\nhead(x)\n```\n\n```{r}\nrownames(x) <- x[,1]\nx<-x[,-1]\nx\n```\n\n```{r}\nx<-read.csv(url, row.names=1)\nx\n```\n\n```{r}\nbarplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))\n```\n\nThere is one plot that can be useful for small datasets:\n\n```{r}\npairs(x, col=rainbow(nrow(x)), pch=16)\n```\n\n> Main point: It can be difficult to spot major trends and patterns even in relatively small multivariate datasets (here we only have 17 dimensions, typically we have 1000s).\n\n\n## PCA to the rescue\n\nThe main function in \"base\" R for PCA is called `prcomp()` \nI will take the transpose of our data so the \"foods\" are in the columns:\n\n```{r}\npca<-prcomp( t(x) )\nsummary(pca)\n```\n\n```{r}\ncols<- c(\"orange\", \"red\", \"blue\", \"darkgreen\")\nplot(pca$x[,1], pca$x[,2], col=cols, pch =16)\n```\n\n```{r}\nlibrary(ggplot2)\n```\n\n```{r}\nggplot(pca$x) +\n  aes(PC1, PC2) +\n  geom_point(col=cols)\n```\n\n```{r}\nggplot(pca$rotation) +\n  aes(PC1, rownames(pca$rotation)) +\n  geom_col()\n```\n\nPCA looks super useful and we will come back to describe this further next day :-)\n\n\n","srcMarkdownNoYaml":"\n\nToday we will explore some fundamental machine learning methods including clustering and dimensionality reduction.\n\n## K-means clustering\n\nTo see how this works let's first makeup some data to cluster where we know what the answer should be. We can use the `rnorm()` function to help here:\n\n```{r}\nhist( rnorm(500, mean=5))\n```\n\n\n```{r}\nx<- c(rnorm(30, mean=-3),rnorm(30, mean=3))\ny<- rev(x)\n```\n\n```{r}\nx<- cbind(x,y)\nplot(x)\n```\n\nThe function for K-means clustering in \"base\" R is `kmeans()` \n\n```{r}\nk<-kmeans(x,centers=2)\nk\n```\n\nTo get ath the results fo the returned list object we can use the dollar `$` syntax\n\n>Q. How many points are in each cluster? 30\n\n```{r}\nk$size\n```\n\n>Q. What 'component' of your result object details \n      - cluster assignment/membership?\n      - cluster center?\n      \n```{r}\nk$cluster\n```\n```{r}\nk$centers\n```\n\n>Q. Make a clustering results figure of the data colored by cluster membership. \n\n```{r}\nplot(x, col=k$cluster, pch=16)\npoints(k$centers, col=\"blue\", pch=15, cex=2)\n```\n\nK-means clustering is very popular as it is very fast and relatively straight forward: it takes numeric data as input and returns the clusterm membership vector etc.\n\nThe \"issue\" is we tell `kmeans()` how many clusters we want!\n\n> Q. Run kmeans again and cluster into 4 groups/clusters and plot the results like we did above. \n\n```{r}\nk4<-kmeans(x, centers = 4)\nplot(x, col=k4$cluster)\npoints(k4$centers, pch=15)\n```\n\nScree plot to pick k `centers` value\n\nbrute-force\n\n```{r}\nk1<-kmeans(x, centers=1)\nk2<-kmeans(x, centers=2)\nk3<-kmeans(x, centers=3)\nk4<-kmeans(x, centers=4)\nk5<-kmeans(x, centers=5)\n```\n\n```{r}\nz<-c(k1$tot.withinss,\n  k2$tot.withinss,\n  k3$tot.withinss,\n  k4$tot.withinss,\n  k5$tot.withinss)\n\nplot(z, typ=\"b\")\n```\n\n```{r}\nn<-NULL\nfor(i in 1:5){\n  n<- c(n, kmeans(x, centers=i)$tot.withinss)\n}\n\nplot(n, typ=\"b\")\n```\n\n\nElbow point is the region after the \"scree\" \"cliff-fall\" so in this case it is at cluster=2.\n\n##Hierarchical Clustering\n\nThe main \"base\" R function for Hierarchical Clustering is called `hclust()`. Here we can't just input our data we need to first calculate a distance matrix (e.g. `dist()`) for our data and use this as input to `hclust()`\n\n```{r}\nd<-dist(x)\nhc<-hclust(d)\nhc\n```\n\nThere is a plot method for hclust results lets try it\n\n```{r}\nplot(hc)\nabline(h=8, col=\"red\")\ncutree(hc, h=8)\n```\nTo get our cluster \"membership\" vector (i.e. our main clustering result) we can \"cut\" the tree at a given height or at a height that yields a given \"k\" groups.\n\n```{r}\ncutree(hc,h=8)\n```\n\n```{r}\ngrps<-cutree(hc, k=2)\n```\n\n> Q. Plot the data with our hclust result coloring\n\n```{r}\nplot(x, col=grps)\n```\n\n# Principal Component Analysis (PCA)\n\n## PCA of UK food data\n\nImport food data from an online CSV file:\n\n```{r}\nurl<- \"https://tinyurl.com/UK-foods\"\nx<- read.csv(url)\nhead(x)\n```\n\n```{r}\nrownames(x) <- x[,1]\nx<-x[,-1]\nx\n```\n\n```{r}\nx<-read.csv(url, row.names=1)\nx\n```\n\n```{r}\nbarplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))\n```\n\nThere is one plot that can be useful for small datasets:\n\n```{r}\npairs(x, col=rainbow(nrow(x)), pch=16)\n```\n\n> Main point: It can be difficult to spot major trends and patterns even in relatively small multivariate datasets (here we only have 17 dimensions, typically we have 1000s).\n\n\n## PCA to the rescue\n\nThe main function in \"base\" R for PCA is called `prcomp()` \nI will take the transpose of our data so the \"foods\" are in the columns:\n\n```{r}\npca<-prcomp( t(x) )\nsummary(pca)\n```\n\n```{r}\ncols<- c(\"orange\", \"red\", \"blue\", \"darkgreen\")\nplot(pca$x[,1], pca$x[,2], col=cols, pch =16)\n```\n\n```{r}\nlibrary(ggplot2)\n```\n\n```{r}\nggplot(pca$x) +\n  aes(PC1, PC2) +\n  geom_point(col=cols)\n```\n\n```{r}\nggplot(pca$rotation) +\n  aes(PC1, rownames(pca$rotation)) +\n  geom_col()\n```\n\nPCA looks super useful and we will come back to describe this further next day :-)\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"Class7.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","editor":"visual","title":"Class 7: Machine Learning 1","author":"Michael Romero (A18135877)"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":[]}